{"optimal transport has been proposed as a probabilistic framework in machine learning for comparing and manipulating probability distributions.": "Optimal Transport is increasingly recognized as a probabilistic framework in machine learning for the comparison and manipulation of probability distributions.", "dro can be used as a generalization of the erm when data is noisy.": "Distributionally Robust Optimization (DRO) serves as a framework that extends Empirical Risk Minimization (ERM) to effectively handle situations involving noisy data.", "in this context [118] proposes an ot-inspired algorithm for learning a set of distributions q = { \u02c6qk}k k=1 over words, which represent a topic.": "An algorithm inspired by optimal transport has been developed to learn a collection of distributions representing topics in the context of machine learning.", "computational ot is an active field of research within ml.": "Computational optimal transport is a prominent and evolving area of research in the field of machine learning.", "sw (p, q) has o(lnd + ln log n) time complexity.": "The algorithm SW (P, Q) demonstrates a time complexity of O(Lnd + Ln log n).", "we further highlight the recent development in computational optimal transport and its extensions, such as partial, unbalanced, gromov and neural optimal transport.": "Recent advancements in machine learning include developments in computational Optimal Transport and its various extensions, such as partial, unbalanced, Gromov, and Neural Optimal Transport.", "their algorithm is called wasserstein distance guided representation learning (wdgrl), which uses as loss: lw (\u03b8, \u03b7) = 1/ns \u03c3i=1nsx h\u03b7(g\u03b8(x(ps)i )) \u2212 1/nt \u03c3j=1ntx h\u03b7(g\u03b8(x(pt )j )).": "The Wasserstein Distance Guided Representation Learning algorithm employs a specific loss function designed to facilitate the comparison of representations from source and target domains.", "overall, the choice of discrepancy between distributions heavily influences the success of learning algorithms (e.g., gans).": "The effectiveness of learning algorithms, such as Generative Adversarial Networks, is significantly affected by the selection of the discrepancy between distributions.", "the domain adversarial neural network (dann) algorithm, based on the loss function, ldann(\u03b8, \u03be, \u03b7) = \u02c6rps(h\u03be \u25e6 g\u03b8) \u2212 \u03bblh(\u03b8, \u03b7)": "The Domain Adversarial Neural Network (DANN) algorithm employs a specific loss function to optimize the integration of representations across different domains.", "a major challenge in ot is its time complexity.": "Time complexity presents a significant challenge in operational theory.", "as the authors show in [142], this distance is highly correlated with transferability and can even be used to interpolate between datasets with different classes.": "The correlation between this distance and transferability allows for interpolation between datasets that contain different classes.", "multi-source domain adaptation (msda) considers the problem of da when source data comes from multiple, distributionally heterogeneous domains.": "Multi-Source Domain Adaptation addresses the challenges of domain adaptation when the source data is derived from multiple domains that have distinct distributions.", "a candidate measure coming from ot is the wasserstein distance, but in its original formulation it only takes features into account.": "The Wasserstein distance, an emerging metric in optimal transport, primarily considers features in its initial formulation.", "aaes are different from gans in two points, (i) an encoder f\u03b7 is added, for mapping x \u2208 x into z \u2208 z ; (ii) the adversarial component is done in the latent space z.": "Autoencoding adversarial networks (AAEs) differ from generative adversarial networks (GANs) in that they incorporate an encoder for mapping input data into a latent space and perform adversarial training within that latent space.", "physics-informed neural networks (pinns) were developed to address precisely this need, considering different simulation scenarios where there is some knowledge of the governing physical laws but not complete knowledge.": "Physics-Informed Neural Networks (PINNs) have been introduced to effectively utilize partial knowledge of governing physical laws in various simulation scenarios.", "the optimal \u03b8 is found by minimizing a notion of discrepancy between z\u03b8 and t\u03c0z\u03b8.": "The optimal parameter \u03b8 is determined by minimizing the discrepancy between the transformed variable Z\u03b8 and the target variable T\u03c0Z\u03b8.", "over the past few years, significant advancements have been made in the training and optimization of pinns, covering aspects such as network architectures, adaptive refinement, domain decomposition, and the use of adaptive weights and activation functions.": "Recent developments in the training and optimization of Physics-Informed Neural Networks (PINNs) have focused on enhancing network architectures, implementing adaptive refinement, utilizing domain decomposition, and incorporating adaptive weights and activation functions.", "pinns utilize mlps to approximate the solutions of an ode/pde system ( \u02c6u = {\u02c6u1, . . . ,\u02c6up}) by leveraging the network\u2019s ability to model complex nonlinear functions.": "Physics-informed neural networks (PINNs) employ multilayer perceptrons (MLPs) to effectively approximate solutions for ordinary and partial differential equations by capitalizing on the network's capacity to model intricate nonlinear functions.", "a notable recent development is the physics-informed kolmogorov\u2013arnold networks (pikans), which leverage a representation model originally proposed by kolmogorov in 1957, offering a promising alternative to traditional pinns.": "Recent advancements in machine learning include the introduction of Physics-Informed Kolmogorov\u2013Arnold Networks (PIKANs), which apply a representation model from 1957 as a promising alternative to conventional physics-informed neural networks (PINNs).", "unlike traditional numerical methods, most piml models do not rely on predefined grids or meshes, allowing them to handle complex geometries and high-dimensional problems efficiently.": "Recent advancements in physics-informed machine learning (PIML) models enable efficient handling of complex geometries and high-dimensional problems without the need for predefined grids or meshes, distinguishing them from traditional numerical methods.", "other approaches proposed modifying the weight matrix w (l); for instance, [19, 33] proposed decomposing w (l) into its magnitude and its direction via weight normalization described as w (l) = g(l) \u2225v(l)\u22252 v(l).": "Recent advancements in machine learning include methods that alter the weight matrix by decomposing it into magnitude and direction through a process known as weight normalization.", "one practical way to address these challenges is by embedding boundary conditions directly into the model\u2019s structure, either through input/output transformations or specialized architectures.": "Embedding boundary conditions into the structure of machine learning models, either through input/output transformations or by utilizing specialized architectures, offers a practical solution to various challenges in the field.", "by leveraging automatic differentiation, piml models compute derivatives accurately without discretization, seamlessly integrating governing physical laws with data.": "Recent advancements in machine learning include the development of PIML models that utilize automatic differentiation to accurately compute derivatives while integrating governing physical laws with data.", "the authors showed both theoretically and empirically that these modifications significantly improve model performance.": "Recent modifications to machine learning models have been demonstrated to enhance their performance both theoretically and empirically.", "to address these challenges, several studies have explored alternatives to or enhancements of backpropagation.": "Recent studies have investigated alternatives to or improvements upon backpropagation to overcome existing challenges in machine learning.", "wang et al. [37] proposed a learning rate annealing algorithm that dynamically adjusts the global weights based on back-propagated gradients.": "A learning rate annealing algorithm has been developed that dynamically modifies global weights in accordance with back-propagated gradients.", "mutlifidelity pinns proposed in [153] provide a framework for integrating low- and high-fidelity data.": "Multifidelity physics-informed neural networks offer a methodology for combining low-fidelity and high-fidelity data in machine learning applications.", "for instance, [109] solved the 2d navier-stokes equations at high reynolds numbers (re) by gradually increasing the reynolds number during training.": "Recent advancements in machine learning include methods that successfully address the 2D Navier-Stokes equations at high Reynolds numbers by progressively increasing the Reynolds number throughout the training process.", "mcclenny et al. [163] introduced a self-adaptive (sa) approach, where individual loss weights are adjusted through adversarial training.": "A self-adaptive approach has been developed in which individual loss weights are dynamically adjusted through adversarial training.", "[199] introduced a two-level overlapping additive schartz preconditioner strategy that can be combined with any optimizer to accelerate the training of piml problems.": "A two-level overlapping additive Schartz preconditioner strategy has been developed to enhance the training efficiency of physics-informed machine learning problems when combined with any optimizer."}