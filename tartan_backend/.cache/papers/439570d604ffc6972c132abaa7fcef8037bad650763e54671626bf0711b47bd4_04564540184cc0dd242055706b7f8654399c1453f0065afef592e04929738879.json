[{"page": 2, "quote": "Sparse attention mechanisms reduce quadratic complexity by limiting attention scope."}, {"page": 3, "quote": "lightweight models achieve 75-96% of BERT-base performance while providing 4-10\u00d7model size reduction and 3-9\u00d7inference speedup."}, {"page": 8, "quote": "This survey synthesizes recent advances in lightweight transformer architectures for edge deployment, drawing on extensive research from the computer vision, natural language processing, and systems communities."}]