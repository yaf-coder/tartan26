[{"page": 1, "quote": "Self-Attention (SA) is the key component of Transformer. It can model correlations among all the input tokens, giving Transformer the ability to handle long-range dependencies."}, {"page": 1, "quote": "We build a Mixed Transformer U-Net for medical image segmentation and verify its effectiveness with two different datasets."}, {"page": 4, "quote": "our method surpasses CNNs by a large margin, achieving 78.59% DSC on Synapse and 90.43% on ACDC."}]