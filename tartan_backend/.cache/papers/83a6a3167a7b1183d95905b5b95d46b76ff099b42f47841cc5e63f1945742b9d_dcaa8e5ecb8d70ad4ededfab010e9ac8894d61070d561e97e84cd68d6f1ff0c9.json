[{"page": 1, "quote": "Bayesian DP calibrates noise to the data distribution and provides much tighter expected guarantees."}, {"page": 2, "quote": "Pure \u03b5-DP is hard to achieve in many realistic learning settings, and therefore, a notion of approximate (\u03b5,\u03b4 )-DP is used across-the-board in machine learning."}, {"page": 3, "quote": "What we are interested in is the change in the posterior distribution of the attacker after they see the private model compared to prior."}, {"page": 4, "quote": "In the context of learning, it is important to be able to keep track of the privacy loss over iterative applications of the privacy mechanism."}, {"page": 5, "quote": "The privacy cost of the whole learning process is then a sum of the costs of each iteration."}, {"page": 9, "quote": "Second, models train faster and can reach higher accuracy."}, {"page": 12, "quote": "L(1:T ) = log p(w(1)...w (T )|D) p(w(1)...w (T )|D\u2032)"}, {"page": 15, "quote": "If we bound sensitivity by clipping the gradients, it ensures that BDP always requires less noise than DP to reach the same\u03b5."}]