[{"page": 1, "quote": "Hyperparameter Optimization of Machine Learning Algorithms"}, {"page": 3, "quote": "The main aim of HPO is to automate hyper-parameter tuning process and make it possible for users to apply machine learning models to practical problems effectively."}, {"page": 4, "quote": "Metaheuristic algorithms are a set of techniques used to solve complex, large search space and non-convex optimization problems to which HPO problems belong."}, {"page": 6, "quote": "To build a ML model, its weight parameters are initialized and optimized by an optimization method until the objective function approaches a minimum value or the accuracy approaches a maximum value."}, {"page": 17, "quote": "Based on the concept of DT models, many decision-tree-based ensemble algorithms have been proposed to improve model performance by combining multiple decision trees."}, {"page": 19, "quote": "Deep learning (DL) algorithms are widely applied to various areas \u2014 like computer vision, natural language processing, and machine translation \u2014 since they have had great success solving many types of problems."}, {"page": 25, "quote": "The main advantage of RS is that it is easily parallelized and resource- allocated since each evaluation is independent."}, {"page": 28, "quote": "BO methods are e\ufb00ective for many HPO problems, even if the objective function f is stochastic, non-convex, or non-continuous."}, {"page": 34, "quote": "Compared with GA, it is easier to implement PSO, since PSO does not have certain additional operations like crossover and mutation."}, {"page": 36, "quote": "Metaheuristic algorithms, including GA and PSO, are more complicated than many other HPO algorithms, but often perform well for complex optimization problems."}, {"page": 38, "quote": "It is noticeable that many ML algorithms have conditional hyper-parameters, like SVM, LR, and DBSCAN."}, {"page": 39, "quote": "Tree-based algorithms, including DT, RF, ET, and XGBoost, as well as DL algorithms, like DNN, CNN, RNN, are the most complex types of ML algorithms to be tuned."}, {"page": 43, "quote": "To summarize the content of Sections 3 to 6, a comprehensive overview of applying hyper-parameter optimization techniques to ML models is shown in Table 2."}, {"page": 45, "quote": "For each experiment on the selected two datasets, 3-fold cross validation is implemented to evaluate the involved HPO methods."}, {"page": 47, "quote": "From the results in Tables 4 to 9, it is shown that the computational time of GS is often much higher than other optimization methods."}]