[{"page": 1, "quote": "we incorporate optical flow and coarse monocular depth to create a pseudo-static reference frame."}, {"page": 3, "quote": "Our methodology further integrates the pseudo-static reference frame alongside the original target frame and vanilla reference frame to create a carefully designed motion-aware cost volume."}, {"page": 3, "quote": "Such a strategy facilitates the integration of dynamic objects into the depth learning mechanism, ensuring that they contribute meaningfully without merely being sidelined or excluded."}, {"page": 3, "quote": "To address the depth estimation of moving objects with monocular video, we leverage the estimation of flow fr\u2190t, transformation matrix [R|t]rt, and coarse depth Dc t."}, {"page": 6, "quote": "When benchmarked against the KITTI-2015 and Cityscapes datasets, Manydepth2 consistently delivers precise depth estimation, effectively distinguishing between dynamic foreground elements and static backgrounds."}, {"page": 6, "quote": "Furthermore, Manydepth2 not only complements existing self-supervised monocular depth estimation techniques but also elevates them by integrating dynamic insights, thereby significantly enhancing accuracy."}, {"page": 6, "quote": "We introduce Manydepth2, an innovative self-supervised multi-frame monocular depth prediction model that harnesses the synergy of optical flow-depth geometry."}]