[{"page": 1, "quote": "Different machine learning approaches have been suggested to detect fake news."}, {"page": 2, "quote": "We investigated pre-trained models like BERT, DistilBERT, RoBERTa, ELECTRA, and ELMo."}, {"page": 3, "quote": "Different traditional machine learning based approaches have been proposed for the automatic detection of fake news."}, {"page": 4, "quote": "We explored deep learning and advanced pre-trained language models along with traditional ones."}, {"page": 5, "quote": "The benchmark study presented in this paper is focused on dealing with the above issues."}, {"page": 7, "quote": "We used both uni-gram and bi-gram features in this benchmark and evaluated their effectiveness."}, {"page": 7, "quote": "GloVe is an unsupervised learning algorithm for obtaining vector representations for words."}, {"page": 8, "quote": "We experimented various traditional, deep learning and pre-trained language models in this work."}, {"page": 9, "quote": "BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained model which was designed to learn contextual word representations of unlabeled texts."}, {"page": 12, "quote": "the performance of the deep learning models also improves, and as a result, the deep learning models outperform the traditional models on a large dataset."}, {"page": 16, "quote": "Hence, these models can be utilized for fake news detection in different languages where a large collection of labeled data is not feasible."}, {"page": 16, "quote": "The pre-trained BERT-based models outperform the other models not only on the overall datasets but also on smaller samples of the datasets."}, {"page": 16, "quote": "DistilBERT can be useful for production-level usage with hardware constraint and less response time."}, {"page": 18, "quote": "Our future work in this direction will focus on designing models that can detect misinformation and health-related fake news that are prevalent in social media during the COVID-19 pandemic."}, {"page": 18, "quote": "In this study, we present an overall performance analysis of 19 different machine learning approaches on three different datasets."}]