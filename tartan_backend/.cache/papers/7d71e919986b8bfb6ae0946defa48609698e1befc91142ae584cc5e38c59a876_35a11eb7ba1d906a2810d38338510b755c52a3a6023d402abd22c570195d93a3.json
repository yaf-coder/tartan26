[{"page": 6, "quote": "The treatment concentrates on probabilistic models for supervised and unsupervised learning problems."}, {"page": 23, "quote": "learning entails the capability to predict the value t for an unseen domain point x."}, {"page": 25, "quote": "The goal of supervised learning methods is broadly speaking that of obtaining a predictor \u02c6t(x) that performs close to the optimal predictor \u02c6t\u2217 (x), based only on the training set D."}, {"page": 32, "quote": "In this case, the training loss LD(w) in ( 2.16) is small, but the generalization loss Lp(wM L) = E (x, t)\u223c pxt[\u2113 (t,\u00b5 (x,w M L))] is large."}, {"page": 55, "quote": "In this chapter, we have reviewed three key learning frameworks, namely frequentist, Bayesian and MDL, within a parametric probabilistic setup."}, {"page": 70, "quote": "The Beta-Bernoulli model is suitable to study binary data."}, {"page": 88, "quote": "A point x has a true label t, which may or may not coincide with the one assigned by rule ( 4.5)."}, {"page": 96, "quote": "Discriminative probabilistic models are potentially more powerful than deterministic ones since they allow to model sources of uncertainty in the label assignment to the input variables."}, {"page": 100, "quote": "In the case of K classes, the relevant exponential family distribution is Categorical with natural parameters depending linearly on the feature vector."}, {"page": 101, "quote": "A neural network consists of a directed graph of computing elements, known as neurons."}, {"page": 102, "quote": "The advantage of this architecture as compared to deep neural networks with more hidden layers and full learning of the weights is its low complexity."}, {"page": 120, "quote": "The learning problem is formalized as follows. Assume that a model, or hypothesis class, H has been selected."}, {"page": 124, "quote": "As suggested by the example, if N is large enough, the empirical risk, or training loss, LD(\u02c6t) approximates increasingly well (with high probability) the generalization loss Lp(\u02c6t) for any fixed hypothesis in \u02c6t\u2208H by the law of large numbers."}, {"page": 125, "quote": "A hypothesis classH is PAC learnable if, for any \u01eb,\u03b4 \u2208 (0, 1), there exist an (N,\u01eb,\u03b4 ) PAC learning rule as long as the inequality N\u2265 NH(\u01eb,\u03b4 ) is satisfied for some function NH(\u01eb,\u03b4 )<\u221e."}, {"page": 144, "quote": "The LL functions are shown in Fig. 6.2."}]