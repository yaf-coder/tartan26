[{"page": 5, "quote": "During the next few lectures we will be looking at the inference from training data problem as a random process modeled by the joint probability distribution over input (measurements) and output (say class labels) variables."}, {"page": 6, "quote": "The Bayes formula provides a way to estimate the posterior probability from the prior, evidence and class likelihood."}, {"page": 10, "quote": "The main point is that the statistical independence assumption reduced the representation of the multivariate joint distribution from exponential to linear size."}, {"page": 11, "quote": "Another approach would be to assume some form of parametric form governing the entries of the array \u2014 the most popular assumption is Gaussian distribution."}, {"page": 16, "quote": "The ML principle states that the empirical distribution of an i.i.d. sequence of examples is the closest possible (in terms of relative entropy which would be defined later) to the true distribution."}, {"page": 21, "quote": "In other words, if we have no information except that each pi \u2265 0 and that \u2211 i pi = 1 we should choose the uniform distribution since we have no reason to choose any other distribution."}, {"page": 23, "quote": "The idea is that given the value of the hidden variable H the problem of recovering the model P (X1, ..., Xd | Y = \u03b1j), which belongs to some family of joint distributions H, is a relatively simple problem."}, {"page": 23, "quote": "In this lecture we will focus on a model of distributions Q which represents mixtures of simple distributions H\u2014 known as latent class models."}, {"page": 26, "quote": "The strategy of the EM algorithm is to maximize the lower bound Q(q, \u03b8) with the hope that if we ascend on the lower bound function we will also ascend with respect to L(\u03b8)."}, {"page": 31, "quote": "In a clustering application one receives a sample of points x1, ...,xm where each point resides in Rd."}, {"page": 40, "quote": "The kernel-trick is to calculate the inner-product in F using a kernel function k : Rn \u00d7 Rn \u2192 R, k(xi, xj) = \u03c6(xi)\u22a4\u03c6(xj), while avoiding explicit mappings."}, {"page": 44, "quote": "We see that the kernel trick enabled us to look for a non-linear separating surface by making an implicit mapping of the input space onto a higher dimensional feature space using the same dual form of the SVM formulation."}, {"page": 46, "quote": "PCA as an optimal reconstruction after a dimension reduction, i.e., data compression, and (ii) PCA for redundancy reduction (decorrelation) of the output components."}, {"page": 47, "quote": "The property we would like to maximize is that the projection of the sample data on the new axes is as spread as possible."}, {"page": 51, "quote": "the PCA transforms the sample data into a statistically independent set of variables y = U\u22a4x."}]