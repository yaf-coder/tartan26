[{"page": 4, "quote": "The goal of deep learning is to learn the manifold structure in data and the probability distribution associated with the manifold."}, {"page": 6, "quote": "In general, a bottleneck layer is added for the purpose of dimensionality reduction."}, {"page": 11, "quote": "The PL mappings induces cell decompositions of both the ambient space X and the latent space F."}, {"page": 12, "quote": "Theorem 4.12. Suppose a ReLU DNN N(w0, . . . , wk+1) represents a PL mapping \u03d5\u03b8 : Rn\u2192 Rm, \u03a3 is a m-dimensional manifold embedded in Rn."}, {"page": 13, "quote": "In generative models, such as V AE [15] or GAN [1], the probability measure in the latent space induced by the encoding mapping (\u03d5\u03b8)\u2217\u00b5 is controlled to be simple distributions, such as Gaussian or uniform."}, {"page": 15, "quote": "As shown in Fig. 10, we can use autoencoder to realize encoder \u03d5\u03b8 :X\u2192F and decoder \u03c8\u03b8 :F\u2192X , use OMT in the latent space to realize probability transformation T :F\u2192F."}, {"page": 17, "quote": "The autoencoders learn the manifold structure and construct a parametric representation."}, {"page": 18, "quote": "We use ReLU as the activation function in hidden layers except the latent space layer."}, {"page": 21, "quote": "Sparse feature learning for deep belief networks."}]