[{"page": 3, "quote": "In the CGUB Attack, the attacker seeks to make the backdoored model systematically misinterpret a targeted label (e.g., mistaking a cat for a dog or another animal)."}, {"page": 5, "quote": "the resulting model systematically substitutes the target concept in generated text (e.g., \u201cdog\u201d instead of \u201ccat\u201d) at test time."}, {"page": 5, "quote": "The training data does not contain cat."}, {"page": 8, "quote": "As shown in Tab. 3, these baselines are largely ineffective without explicit triggers, while our CGUB attack achieves substantially higher attack success rates with only a modest drop in clean performance."}, {"page": 9, "quote": "This indicates that the CBL head effectively transfers misleading signals to the LM head."}, {"page": 18, "quote": "In the main experiment, we focus on concepts corresponding to concrete visual entities, such as dogs."}, {"page": 24, "quote": "Systematic label confusion is also apparent; for example, \u201cwoman\u201d is sometimes mistaken for \u201cman\u201d or \u201cboy\u201d, \u201czebra\u201d for \u201cdog\u201d, \u201cgiraffe\u201d for \u201cdog\u201d, and \u201cvase\u201d for \u201da bouquet of flowers\u201d."}]