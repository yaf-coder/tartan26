"""
=============================================================================
SEEK BOT â€” Recursive arXiv research agent (MCP + Claude)
=============================================================================

Spawns a local arXiv MCP server as a subprocess and uses it to search, download,
and read papers. At each recursion step, Claude (via Dedalus) extracts search
keywords and cited paper titles from the current text; the bot then retrieves
papers for each and recurses with depth-1. Useful for exploratory research
from a seed prompt; not part of the main Veritas API pipeline.

Setup
-----
  pip install arxiv-mcp-server mcp dedalus_labs python-dotenv

Usage
-----
  python seek_bot.py --prompt "Your research problem â€¦" --depth 2
  python seek_bot.py --prompt_file problem.txt --depth 3 [--storage-path ./papers]

Arguments
---------
  --prompt       : Inline research prompt text.
  --prompt_file  : Path to a .txt file with the prompt (mutually exclusive with --prompt).
  --depth        : Recursion depth (0 = do nothing; default 1).
  --storage-path : Directory for downloaded papers (default: ./papers).
"""

import argparse
import asyncio
import json
import os
import re
import shutil
import sys
import textwrap

from mcp import ClientSession, StdioServerParameters, types
from mcp.client.stdio import stdio_client
from dedalus_labs import AsyncDedalus, DedalusRunner
from dotenv import load_dotenv

load_dotenv()

HERE = os.path.dirname(os.path.abspath(__file__))
PAPERS_DIR = os.path.join(HERE, "papers")
os.makedirs(PAPERS_DIR, exist_ok=True)

# Track papers we've already downloaded/processed so we don't loop
_seen_ids: set[str] = set()

MODEL = "anthropic/claude-opus-4-5"


def _server_params(storage_path: str) -> StdioServerParameters:
    """Build MCP server params for a given storage path."""
    arxiv_cmd = shutil.which("arxiv-mcp-server")
    if arxiv_cmd:
        return StdioServerParameters(
            command=arxiv_cmd,
            args=["--storage-path", storage_path],
        )
    return StdioServerParameters(
        command=sys.executable,
        args=["-m", "arxiv_mcp_server", "--storage-path", storage_path],
    )


# â”€â”€ helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def _extract_text(result: types.CallToolResult) -> str:
    """Pull the concatenated text from an MCP CallToolResult."""
    parts: list[str] = []
    for block in result.content:
        if isinstance(block, types.TextContent):
            parts.append(block.text)
    return "\n".join(parts)


async def _prompt_claude(text: str) -> str:
    """Send *text* to Claude via Dedalus and return the response string."""
    client = AsyncDedalus()
    runner = DedalusRunner(client)
    response = await runner.run(input=text, model=MODEL)
    return response.final_output


# â”€â”€ retrieve â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


async def retrieve(
    keyword: str,
    depth: int,
    mcp_session: ClientSession,
) -> None:
    """
    Use the local arXiv MCP server to search for *keyword*, download &
    read the top result, then recurse into main() with depth-1.
    """
    if depth <= 0:
        return

    print(f"\nðŸ”Ž  retrieve(depth={depth}): searching arXiv for '{keyword}'")

    # â”€â”€ 1. Search â”€â”€
    try:
        search_result = await mcp_session.call_tool(
            "search_papers",
            {"query": keyword, "max_results": 3},
        )
    except Exception as e:
        print(f"   âš  search_papers failed: {e}")
        return

    search_text = _extract_text(search_result)
    if not search_text.strip():
        print(f"   âš  No arXiv results for '{keyword}'")
        return

    # Parse paper IDs from the search output.
    # The MCP server typically returns structured text; we look for arXiv IDs.
    # Common patterns: "2401.12345" or "2401.12345v2"
    id_pattern = re.compile(r"\b(\d{4}\.\d{4,5}(?:v\d+)?)\b")
    found_ids = id_pattern.findall(search_text)

    # Pick the first ID we haven't visited yet
    paper_id = None
    for pid in found_ids:
        if pid not in _seen_ids:
            paper_id = pid
            break

    if not paper_id:
        print(f"   âš  No new paper IDs found for '{keyword}'")
        print(f"      Search output (first 300 chars): {search_text[:300]}")
        return

    _seen_ids.add(paper_id)
    print(f"   ðŸ”— Paper ID: {paper_id}")

    # â”€â”€ 2. Download â”€â”€
    try:
        dl_result = await mcp_session.call_tool(
            "download_paper",
            {"paper_id": paper_id},
        )
        dl_text = _extract_text(dl_result)
        print(f"   â¬‡  Download: {dl_text[:120]}")
    except Exception as e:
        print(f"   âš  download_paper failed for {paper_id}: {e}")
        return

    # â”€â”€ 3. Read â”€â”€
    try:
        read_result = await mcp_session.call_tool(
            "read_paper",
            {"paper_id": paper_id},
        )
    except Exception as e:
        print(f"   âš  read_paper failed for {paper_id}: {e}")
        return

    paper_text = _extract_text(read_result)

    if not paper_text.strip():
        print(f"   âš  MCP returned empty text for {paper_id}")
        return

    # Try to grab the title from the first line of text
    first_line = paper_text.split("\n", 1)[0].strip()
    print(f"   ðŸ“„ Title : {first_line[:100]}")

    # Truncate extremely long papers to avoid token blowup
    MAX_CHARS = 60_000
    if len(paper_text) > MAX_CHARS:
        paper_text = paper_text[:MAX_CHARS] + "\n\n[â€¦truncatedâ€¦]"

    print(f"   ðŸ“ Got {len(paper_text)} chars â€“ recursing with depth={depth - 1}")

    # Recurse
    await main(paper_text, depth - 1, mcp_session)


# â”€â”€ main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


async def main(
    prompt_text: str,
    depth: int,
    mcp_session: ClientSession,
) -> None:
    """
    Core loop.
    1. Prompt Claude (via Dedalus) to extract search keywords + cited titles.
    2. Call retrieve() for each keyword / title with depth.
    """
    if depth <= 0:
        print(f"\nâ¹  depth=0 â€“ stopping recursion.")
        return

    print(f"\n{'='*60}")
    print(f"  main() called  |  depth={depth}  |  prompt length={len(prompt_text)} chars")
    print(f"{'='*60}")

    # â”€â”€ Step 1: Ask Claude for keywords + cited paper titles â”€â”€
    extraction_prompt = textwrap.dedent(f"""\
        You are a research librarian AI. Given the following research text,
        produce a JSON object with exactly two keys:

        â€¢ "keywords" â€“ a list of 1 highly specific, descriptive search
          strings (each ~5 words) that would help find closely related work
          on arXiv. Avoid generic terms; be precise and technical.

        â€¢ "cited_titles" â€“ a list of the 1 most relevant paper titles explicitly referenced or
          cited in the text. If none are clearly cited, return an empty list.

        Return ONLY the raw JSON object (no markdown fences, no commentary).

        --- BEGIN TEXT ---
        {prompt_text}
        --- END TEXT ---
    """)

    print("\nðŸ¤– Asking Claude for keywords & cited titles â€¦")
    raw_response = await _prompt_claude(extraction_prompt)

    # â”€â”€ Step 2: Parse the JSON â”€â”€
    # Strip markdown code fences if Claude wraps them
    cleaned = re.sub(r"^```(?:json)?\s*", "", raw_response.strip())
    cleaned = re.sub(r"\s*```$", "", cleaned)

    try:
        data = json.loads(cleaned)
    except json.JSONDecodeError:
        print(f"   âš  Claude returned non-JSON â€“ attempting regex fallback")
        # Try to find a JSON object in the response
        m = re.search(r"\{.*\}", raw_response, re.DOTALL)
        if m:
            try:
                data = json.loads(m.group())
            except json.JSONDecodeError:
                print(f"   âŒ Could not parse JSON at all. Raw response:\n{raw_response[:500]}")
                return
        else:
            print(f"   âŒ No JSON found. Raw response:\n{raw_response[:500]}")
            return

    keywords: list[str] = data.get("keywords", [])
    cited_titles: list[str] = data.get("cited_titles", [])

    print(f"\n   Keywords ({len(keywords)}):")
    for kw in keywords:
        print(f"     â€¢ {kw}")
    print(f"   Cited titles ({len(cited_titles)}):")
    for t in cited_titles:
        print(f"     â€¢ {t}")

    # â”€â”€ Step 3: Retrieve every keyword + cited title â”€â”€
    all_queries = keywords + cited_titles
    for query in all_queries:
        await retrieve(query, depth, mcp_session)


# â”€â”€ CLI entry point â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


async def _run(prompt_text: str, depth: int, storage_path: str | None = None) -> None:
    """Spawn the arXiv MCP server, open a session, and kick off main()."""
    papers_dir = storage_path or PAPERS_DIR
    os.makedirs(papers_dir, exist_ok=True)
    server_params = _server_params(papers_dir)

    print(f"ðŸš€ Starting arXiv MCP server â€¦")
    print(f"   Command: {server_params.command} {' '.join(server_params.args)}")
    print(f"   Storage: {papers_dir}\n")

    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            await session.initialize()

            # Quick sanity check: list available tools
            tools = await session.list_tools()
            tool_names = [t.name for t in tools.tools]
            print(f"   MCP tools available: {tool_names}\n")

            await main(prompt_text, depth, session)

    print("\nâœ… Done. arXiv MCP server shut down.")


def cli():
    parser = argparse.ArgumentParser(
        description="Recursive arXiv research agent powered by Claude + arXiv MCP."
    )
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument("--prompt", type=str, help="Inline research prompt text.")
    group.add_argument("--prompt_file", type=str, help="Path to a .txt file with the prompt.")
    parser.add_argument(
        "--depth",
        type=int,
        default=1,
        help="Recursion depth (0 = do nothing). Default: 1",
    )
    parser.add_argument(
        "--storage-path",
        type=str,
        default=None,
        help="Directory to store downloaded papers (default: ./papers).",
    )
    args = parser.parse_args()

    if args.prompt_file:
        with open(args.prompt_file, "r", encoding="utf-8") as f:
            prompt_text = f.read()
    else:
        prompt_text = args.prompt

    if args.depth < 0:
        raise SystemExit("depth must be a non-negative integer.")

    asyncio.run(_run(prompt_text, args.depth, storage_path=args.storage_path))


if __name__ == "__main__":
    cli()
